{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ec8da182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4dce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validasi Selesai.\n",
      "Data dibuang karena tidak relevan (Lexicon Conflict): 3424 baris.\n",
      "Total Data Setelah Validasi Lexicon: 8576\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('clash_royale_reviews.csv')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def validate_label_with_lexicon(row):\n",
    "    score = row['score']\n",
    "    text = str(row['content'])\n",
    "    vader_score = sia.polarity_scores(text)['compound']\n",
    "    \n",
    "    if score <= 2:\n",
    "        if vader_score > 0.5:\n",
    "            return 'invalid'\n",
    "        return 0\n",
    "        \n",
    "    elif score == 3:\n",
    "        return 1 \n",
    "        \n",
    "    elif score >= 4:\n",
    "        if vader_score < -0.5: \n",
    "            return 'invalid'\n",
    "        return 2\n",
    "\n",
    "df['validated_label'] = df.apply(validate_label_with_lexicon, axis=1)\n",
    "\n",
    "initial_len = len(df)\n",
    "df = df[df['validated_label'] != 'invalid'].copy()\n",
    "df['label'] = df['validated_label'].astype(int)\n",
    "\n",
    "print(f\"Validasi Selesai.\")\n",
    "print(f\"Data dibuang karena tidak relevan (Lexicon Conflict): {initial_len - len(df)} baris.\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text_deep(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) \n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_text_deep)\n",
    "print(f\"Total Data Setelah Validasi Lexicon: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e06d871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Final: 12000 (3.500 per kelas)\n"
     ]
    }
   ],
   "source": [
    "target_size = 4000\n",
    "\n",
    "df_0 = df[df['label'] == 0]\n",
    "df_1 = df[df['label'] == 1]\n",
    "df_2 = df[df['label'] == 2]\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_0_bal = resample(df_0, replace=False, n_samples=target_size, random_state=42)\n",
    "df_1_bal = resample(df_1, replace=True, n_samples=target_size, random_state=42)\n",
    "df_2_bal = resample(df_2, replace=True, n_samples=target_size, random_state=42)\n",
    "\n",
    "df_final = pd.concat([df_0_bal, df_1_bal, df_2_bal])\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "Y_labels = df_final['label'].values \n",
    "Y_onehot = pd.get_dummies(df_final['label']).values\n",
    "\n",
    "print(f\"Total Data Final: {len(df_final)} (3.500 per kelas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a6146c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SKEMA 1: SVM (Traditional ML) + TF-IDF ===\n",
      "Sedang melatih SVM... (Mohon tunggu)\n",
      "\n",
      "Akurasi Training Set: 93.83%\n",
      "Akurasi Testing Set : 86.12%\n",
      "✅ Kriteria Machine Learning Tradisional (>85%) TERCAPAI!\n",
      "\n",
      "Classification Report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       0.86      0.84      0.85       791\n",
      "      Netral       0.84      0.88      0.86       833\n",
      "     Positif       0.89      0.86      0.87       776\n",
      "\n",
      "    accuracy                           0.86      2400\n",
      "   macro avg       0.86      0.86      0.86      2400\n",
      "weighted avg       0.86      0.86      0.86      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== SKEMA 1: SVM (Traditional ML) + TF-IDF ===\")\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf.fit_transform(df_final['clean_content']).toarray()\n",
    "\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n",
    "    X_tfidf, Y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=1.5, random_state=42)\n",
    "\n",
    "print(\"Sedang melatih SVM... (Mohon tunggu)\")\n",
    "svm_model.fit(X_train_svm, y_train_svm)\n",
    "y_pred_svm = svm_model.predict(X_test_svm)\n",
    "acc_train_svm = svm_model.score(X_train_svm, y_train_svm)\n",
    "acc_test_svm = accuracy_score(y_test_svm, y_pred_svm)\n",
    "\n",
    "print(f\"\\nAkurasi Training Set: {acc_train_svm*100:.2f}%\")\n",
    "print(f\"Akurasi Testing Set : {acc_test_svm*100:.2f}%\")\n",
    "\n",
    "if acc_test_svm > 0.85:\n",
    "    print(\"✅ Kriteria Machine Learning Tradisional (>85%) TERCAPAI!\")\n",
    "else:\n",
    "    print(\"⚠️ Coba tuning parameter C pada SVM.\")\n",
    "\n",
    "print(\"\\nClassification Report (SVM):\")\n",
    "print(classification_report(y_test_svm, y_pred_svm, target_names=['Negatif', 'Netral', 'Positif']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dc9fb2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PERSIAPAN SEQUENCE (Skema 2 & 3) ===\n",
      "Tokenizer siap. Data teks telah diubah menjadi sequence angka.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== PERSIAPAN SEQUENCE (Skema 2 & 3) ===\")\n",
    "\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 120\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df_final['clean_content'])\n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(df_final['clean_content'])\n",
    "X_pad = pad_sequences(X_seq, maxlen=MAX_LEN)\n",
    "\n",
    "print(\"Tokenizer siap. Data teks telah diubah menjadi sequence angka.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "811b92d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SKEMA 2: CNN 1D + Embedding (80:20) ===\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5708 - loss: 0.9047 - val_accuracy: 0.7833 - val_loss: 0.6108\n",
      "Epoch 2/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8481 - loss: 0.4430 - val_accuracy: 0.8888 - val_loss: 0.3500\n",
      "Epoch 3/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9559 - loss: 0.1656 - val_accuracy: 0.9250 - val_loss: 0.2447\n",
      "Epoch 4/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9873 - loss: 0.0601 - val_accuracy: 0.9304 - val_loss: 0.2446\n",
      "Epoch 5/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9959 - loss: 0.0260 - val_accuracy: 0.9300 - val_loss: 0.2656\n",
      "Epoch 6/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9973 - loss: 0.0150 - val_accuracy: 0.9312 - val_loss: 0.2837\n",
      "Epoch 7/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0077 - val_accuracy: 0.9262 - val_loss: 0.3194\n",
      "Epoch 8/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.9994 - loss: 0.0063 - val_accuracy: 0.9283 - val_loss: 0.3299\n",
      "Epoch 9/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9997 - loss: 0.0043 - val_accuracy: 0.9254 - val_loss: 0.3616\n",
      "Epoch 10/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9998 - loss: 0.0030 - val_accuracy: 0.9283 - val_loss: 0.3625\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SKEMA 2: CNN 1D + Embedding (80:20) ===\")\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_pad, Y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(MAX_WORDS, 100, input_length=MAX_LEN))\n",
    "model2.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())                                  \n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(X_train_2, y_train_2, epochs=10, batch_size=64, validation_data=(X_test_2, y_test_2), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d141e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUASI SKEMA 2 (CNN 1D) ===\n",
      "Akurasi Training Set: 100.00%\n",
      "Akurasi Testing Set : 92.83%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       0.93      0.88      0.91       791\n",
      "      Netral       0.94      0.97      0.96       833\n",
      "     Positif       0.91      0.93      0.92       776\n",
      "\n",
      "    accuracy                           0.93      2400\n",
      "   macro avg       0.93      0.93      0.93      2400\n",
      "weighted avg       0.93      0.93      0.93      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EVALUASI SKEMA 2 (CNN 1D) ===\")\n",
    "\n",
    "loss_train2, acc_train2 = model2.evaluate(X_train_2, y_train_2, verbose=0)\n",
    "loss_test2, acc_test2 = model2.evaluate(X_test_2, y_test_2, verbose=0)\n",
    "\n",
    "print(f\"Akurasi Training Set: {acc_train2*100:.2f}%\")\n",
    "print(f\"Akurasi Testing Set : {acc_test2*100:.2f}%\")\n",
    "\n",
    "y_pred_2 = np.argmax(model2.predict(X_test_2, verbose=0), axis=1)\n",
    "y_true_2 = np.argmax(y_test_2, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_2, y_pred_2, target_names=['Negatif', 'Netral', 'Positif']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9b121f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SKEMA 3: Hybrid CNN + LSTM (85:15) ===\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.5102 - loss: 0.9797 - val_accuracy: 0.6550 - val_loss: 0.7702\n",
      "Epoch 2/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.7396 - loss: 0.6447 - val_accuracy: 0.7906 - val_loss: 0.5562\n",
      "Epoch 3/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.8639 - loss: 0.4018 - val_accuracy: 0.8628 - val_loss: 0.4207\n",
      "Epoch 4/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9260 - loss: 0.2333 - val_accuracy: 0.8861 - val_loss: 0.3464\n",
      "Epoch 5/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9559 - loss: 0.1480 - val_accuracy: 0.8983 - val_loss: 0.3376\n",
      "Epoch 6/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9750 - loss: 0.0897 - val_accuracy: 0.9044 - val_loss: 0.3407\n",
      "Epoch 7/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9841 - loss: 0.0590 - val_accuracy: 0.8961 - val_loss: 0.3927\n",
      "Epoch 8/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9871 - loss: 0.0448 - val_accuracy: 0.9139 - val_loss: 0.3583\n",
      "Epoch 9/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9915 - loss: 0.0323 - val_accuracy: 0.9122 - val_loss: 0.3842\n",
      "Epoch 10/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9935 - loss: 0.0214 - val_accuracy: 0.9089 - val_loss: 0.4270\n",
      "Epoch 11/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9953 - loss: 0.0169 - val_accuracy: 0.9150 - val_loss: 0.4179\n",
      "Epoch 12/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9962 - loss: 0.0139 - val_accuracy: 0.9117 - val_loss: 0.4684\n",
      "Epoch 13/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9965 - loss: 0.0114 - val_accuracy: 0.9094 - val_loss: 0.4776\n",
      "Epoch 14/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9968 - loss: 0.0103 - val_accuracy: 0.9122 - val_loss: 0.4693\n",
      "Epoch 15/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9972 - loss: 0.0096 - val_accuracy: 0.9122 - val_loss: 0.5022\n",
      "Epoch 16/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9978 - loss: 0.0085 - val_accuracy: 0.9217 - val_loss: 0.4813\n",
      "Epoch 17/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9981 - loss: 0.0072 - val_accuracy: 0.9133 - val_loss: 0.5389\n",
      "Epoch 18/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9987 - loss: 0.0049 - val_accuracy: 0.9111 - val_loss: 0.5756\n",
      "Epoch 19/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9986 - loss: 0.0060 - val_accuracy: 0.9000 - val_loss: 0.6258\n",
      "Epoch 20/20\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9971 - loss: 0.0090 - val_accuracy: 0.9217 - val_loss: 0.5234\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SKEMA 3: Hybrid CNN + LSTM (85:15) ===\")\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_pad, Y_onehot, test_size=0.15, random_state=555)\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(MAX_WORDS, 100, input_length=MAX_LEN))\n",
    "model3.add(SpatialDropout1D(0.4))\n",
    "model3.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model3.add(GlobalMaxPooling1D())\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0005)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_hybrid.keras\", monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "history3 = model3.fit(X_train_3, y_train_3, epochs=20, batch_size=32, validation_data=(X_test_3, y_test_3), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8f1eca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUASI SKEMA 3 (HYBRID) ===\n",
      "Akurasi Training Set: 100.00%\n",
      "Akurasi Testing Set : 92.17%\n",
      "✅ TARGET HIGH SCORE TERCAPAI (>92%)\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       0.93      0.87      0.90       595\n",
      "      Netral       0.93      0.96      0.95       581\n",
      "     Positif       0.90      0.94      0.92       624\n",
      "\n",
      "    accuracy                           0.92      1800\n",
      "   macro avg       0.92      0.92      0.92      1800\n",
      "weighted avg       0.92      0.92      0.92      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== EVALUASI SKEMA 3 (HYBRID) ===\")\n",
    "model3.load_weights(\"model_hybrid.keras\")\n",
    "\n",
    "loss_train3, acc_train3 = model3.evaluate(X_train_3, y_train_3, verbose=0)\n",
    "loss_test3, acc_test3 = model3.evaluate(X_test_3, y_test_3, verbose=0)\n",
    "\n",
    "print(f\"Akurasi Training Set: {acc_train3*100:.2f}%\")\n",
    "print(f\"Akurasi Testing Set : {acc_test3*100:.2f}%\")\n",
    "\n",
    "if acc_test3 > 0.92:\n",
    "    print(\"✅ TARGET HIGH SCORE TERCAPAI (>92%)\")\n",
    "else:\n",
    "    print(\"⚠️ Masih di bawah target, coba run ulang.\")\n",
    "\n",
    "y_pred_3 = np.argmax(model3.predict(X_test_3, verbose=0), axis=1)\n",
    "y_true_3 = np.argmax(y_test_3, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_3, y_pred_3, target_names=['Negatif', 'Netral', 'Positif']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52b3df4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model 2 berhasil disimpan sebagai 'model_cnn_clash_royale.keras'\n",
      "✅ Tokenizer berhasil disimpan sebagai 'tokenizer.pickle'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 1. Simpan Model 2 (CNN 1D)\n",
    "model2.save('model_cnn_clash_royale.keras')\n",
    "print(\"✅ Model 2 berhasil disimpan sebagai 'model_cnn_clash_royale.keras'\")\n",
    "\n",
    "# 2. Simpan Tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"✅ Tokenizer berhasil disimpan sebagai 'tokenizer.pickle'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
